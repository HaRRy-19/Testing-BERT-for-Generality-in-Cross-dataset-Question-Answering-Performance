%tensorflow_version 2.x
!pip install tokenizers
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))
# Mount google drive

from google.colab import drive
drive.mount('/content/gdrive')


# Write a test file to test the connection

with open('/content/gdrive/My Drive/TrainingResults/file.txt', 'w') as f:
  f.write('content')
TRAIN BERT ON DEFAULT SQUAD 1.1 DATABASE
# Imports
import json
import os
import re
import string
import numpy as np
import tensorflow_hub as hub
from tensorflow import keras
from tensorflow.keras import layers
from tokenizers import BertWordPieceTokenizer

# ============================================= PREPARING DATASET ======================================================
class Sample:

    def __init__(self, question, context, id, start_char_idx=None, answer_text=None, all_answers=None):
        self.question = question
        self.context = context
        self.start_char_idx = start_char_idx
        self.answer_text = answer_text
        self.all_answers = all_answers
        self.skip = False
        self.start_token_idx = -1
        self.end_token_idx = -1
        self.id = id


    def preprocess(self):
        context = " ".join(str(self.context).split())
        question = " ".join(str(self.question).split())
        tokenized_context = tokenizer.encode(context)
        tokenized_question = tokenizer.encode(question)
        if self.answer_text is not None:
            answer = " ".join(str(self.answer_text).split())
            end_char_idx = self.start_char_idx + len(answer)
            if end_char_idx >= len(context):
                self.skip = True
                return
            is_char_in_ans = [0] * len(context)
            for idx in range(self.start_char_idx, end_char_idx):
                is_char_in_ans[idx] = 1
            ans_token_idx = []
            for idx, (start, end) in enumerate(tokenized_context.offsets):
                if sum(is_char_in_ans[start:end]) > 0:
                    ans_token_idx.append(idx)
            if len(ans_token_idx) == 0:
                self.skip = True
                return
            self.start_token_idx = ans_token_idx[0]
            self.end_token_idx = ans_token_idx[-1]
        input_ids = tokenized_context.ids + tokenized_question.ids[1:]
        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])
        attention_mask = [1] * len(input_ids)
        padding_length = max_seq_length - len(input_ids)
        if padding_length > 0:
            input_ids = input_ids + ([0] * padding_length)
            attention_mask = attention_mask + ([0] * padding_length)
            token_type_ids = token_type_ids + ([0] * padding_length)
        elif padding_length < 0:
            self.skip = True
            return
        self.input_word_ids = input_ids
        self.input_type_ids = token_type_ids
        self.input_mask = attention_mask
        self.context_token_to_char = tokenized_context.offsets

def create_coqa_examples(raw_data):
  coqa_examples = []
  for item in raw_data["data"]:
      context = item["story"]
      idStub = item["id"]
      assert len(item["questions"]) == len(item["answers"])
      for i in range(len(item["questions"])):
        question = item["questions"][i]
        answer = item["answers"][i]
        id = str(idStub) + "_" + str(question["turn_id"])
        assert answer["turn_id"] == question["turn_id"]
        all_answers = [answer["input_text"], answer["span_text"]]
        if (answer["input_text"].lower() in answer["span_text"].lower()):
          ans_start = answer["span_start"] + answer["span_text"].lower().index(answer["input_text"].lower())
        else:
          ans_start = answer["span_start"]
        coqa_eg = Sample(question["input_text"], context, id, ans_start, answer["input_text"], all_answers)
        coqa_eg.preprocess()
        coqa_examples.append(coqa_eg)
  return coqa_examples


def create_squadHotPot_examples(raw_data):
    squad_examples = []
    failToFindCorrectAnswer = 0
    for item in raw_data["data"]:
        for para in item["paragraphs"]:
            context = para["context"]
            if (isinstance(context, list)):
              context = context[0]
            for qa in para["qas"]:
                question = qa["question"]
                id = qa["id"]
                if "answers" in qa:
                    if "is_impossible" in qa and qa["is_impossible"] == True:
                      answer_text = "Unanswerable"
                      all_answers = ["Unanswerable"]
                      start_char_idx = 0
                    else:
                      ## Sometimes the first answer list item is an empty list
                      ## so we iterate over all of them, and the first answer that we find is the main, all others are added to the all_answers variable
                      answer_text = None
                      start_char_idx = None
                      all_answers = []
                      for anstemp in qa["answers"]:
                        if (len(anstemp) == 0):
                          continue
                        ans = anstemp[0]
                        if "text" not in ans: 
                          continue
                        if answer_text == None:
                          answer_text = ans["text"]
                          if ans["answer_start"] > 0:
                            start_char_idx = ans["answer_start"]
                          else:
                            start_char_idx = 0
                        all_answers.append(ans["text"])
                    if answer_text == None and "final_answers" in qa and len(qa["final_answers"]) > 0:
                      all_answers = qa["final_answers"]
                      answer_text = all_answers[0]
                      start_char_idx = 0
                      #if (answer_text.lower() in context.lower()):
                      #  start_char_idx = context.lower().index(answer_text.lower())

                    if answer_text == None:
                      # No valid answer found
                      failToFindCorrectAnswer += 1
                      squad_eg = Sample(question, context, id)
                    else:
                      squad_eg = Sample(question, context, id, start_char_idx, answer_text, all_answers)
                else:
                    squad_eg = Sample(question, context, id)
                squad_eg.preprocess()
                squad_examples.append(squad_eg)
    print("Total samples: ", len(squad_examples))
    print("Failed to find correct answer: ", failToFindCorrectAnswer)
    return squad_examples

def create_squad_examples(raw_data):
    squad_examples = []
    for item in raw_data["data"]:
        for para in item["paragraphs"]:
            context = para["context"]
            for qa in para["qas"]:
                question = qa["question"]
                id = qa["id"]
                if "answers" in qa:
                    if "is_impossible" in qa and qa["is_impossible"] == True:
                      answer_text = "Unanswerable"
                      all_answers = ["Unanswerable"]
                      start_char_idx = -1
                    else:
                      answer_text = qa["answers"][0]["text"]
                      all_answers = [_["text"] for _ in qa["answers"]]
                      start_char_idx = qa["answers"][0]["answer_start"]
                    squad_eg = Sample(question, context, id, start_char_idx, answer_text, all_answers)
                else:
                    squad_eg = Sample(question, context, id)
                squad_eg.preprocess()
                squad_examples.append(squad_eg)
    return squad_examples

def create_inputs_targets(squad_examples):
    dataset_dict = {
        "input_word_ids": [],
        "input_type_ids": [],
        "input_mask": [],
        "start_token_idx": [],
        "end_token_idx": [],
    }
    for item in squad_examples:
        if item.skip == False:
            for key in dataset_dict:
                dataset_dict[key].append(getattr(item, key))
    for key in dataset_dict:
        dataset_dict[key] = np.array(dataset_dict[key])
    x = [dataset_dict["input_word_ids"],
         dataset_dict["input_mask"],
         dataset_dict["input_type_ids"]]
    y = [dataset_dict["start_token_idx"], dataset_dict["end_token_idx"]]
    return x, y

def samples_to_predictions(samples):
  results = {}
  for sample in samples:
    if (sample.start_char_idx == -1): # Question predicted as unanswerable
      results[sample.id] = ""
    else:
      results[sample.id] = sample.answer_text
  return results

def samples_to_squad_json(samples):
  ContextQuestionPairs = {}
  for sample in samples:
    QAPair = {}
    QAPair["id"] = sample.id
    QAPair["question"] = sample.question
    QAPair["answers"] = []
    if (sample.start_char_idx == -1):
      QAPair["is_impossible"] = True
    else:  
      QAPair["answers"].append({"text":sample.answer_text, "answer_start":sample.start_char_idx })
      QAPair["is_impossible"] = False
    if sample.context in ContextQuestionPairs:
      ContextQuestionPairs[sample.context].append(QAPair)
    else:
      qas = [QAPair]
      ContextQuestionPairs[sample.context] = qas
  
  raw_data={}
  data = []
  
  for context, qas in ContextQuestionPairs.items():
    data.append({"title": "", "paragraphs": [{"context": context, "qas": qas}]})
  raw_data["data"] = data
  return raw_data
# =================================================== TRAINING =========================================================


class ValidationCallback(keras.callbacks.Callback):

    def normalize_text(self, text):
        text = text.lower()
        text = "".join(ch for ch in text if ch not in set(string.punctuation))
        regex = re.compile(r"\b(a|an|the)\b", re.UNICODE)
        text = re.sub(regex, " ", text)
        text = " ".join(text.split())
        return text

    def __init__(self, x_eval, y_eval):
        self.x_eval = x_eval
        self.y_eval = y_eval

    def on_epoch_end(self, epoch, logs=None):
        pred_start, pred_end = self.model.predict(self.x_eval)
        count = 0
        eval_examples_no_skip = [_ for _ in eval_squad_examples if _.skip == False]
        for idx, (start, end) in enumerate(zip(pred_start, pred_end)):
            squad_eg = eval_examples_no_skip[idx]
            offsets = squad_eg.context_token_to_char
            start = np.argmax(start)
            end = np.argmax(end)
            if start >= len(offsets):
                continue
            pred_char_start = offsets[start][0]
            if end < len(offsets):
                pred_char_end = offsets[end][1]
                pred_ans = squad_eg.context[pred_char_start:pred_char_end]
            else:
                pred_ans = squad_eg.context[pred_char_start:]
            normalized_pred_ans = self.normalize_text(pred_ans)
            normalized_true_ans = [self.normalize_text(_) for _ in squad_eg.all_answers]
            if normalized_pred_ans in normalized_true_ans:
                count += 1
        acc = count / len(self.y_eval[0])
        print(f"\nepoch={epoch + 1}, exact match score={acc:.2f}")
# Download and open dataset
parser = "SQUAD"
# SQUAD 1.1
train_path = keras.utils.get_file("train.json", "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json")
eval_path = keras.utils.get_file("eval.json", "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json")

# SQUAD 2.0
# train_path = keras.utils.get_file("train.json", "https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json")
# eval_path = keras.utils.get_file("eval.json", "https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json")

# CoQa
#train_path = keras.utils.get_file("train.json", "https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json")
#eval_path = keras.utils.get_file("eval.json", "https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json")
#parser = "COQA"

# HotpotQA
# train_path = "/content/gdrive/My Drive/DataSets/HotpotInSquadFormat/train.json"
# eval_path = "/content/gdrive/My Drive/DataSets/HotpotInSquadFormat/dev.json"
# parser = "HOTPOT"


with open(train_path) as f: raw_train_data = json.load(f)
with open(eval_path) as f: raw_eval_data = json.load(f)
max_seq_length = 512

print(raw_eval_data["data"][0])









input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')
input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_mask')
input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')
bert_layer = hub.KerasLayer("https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2", trainable=True)
pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, input_type_ids])
vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy().decode("utf-8")
do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()
tokenizer = BertWordPieceTokenizer(vocab=vocab_file, lowercase=True)


## Use the proper sample creation algorithm, (HotpotQA has been converted to squad format, so uses the squad algorithm)
# if (parser == "COQA"):
#   train_squad_examples = create_coqa_examples(raw_train_data)
# elif (parser == "HOTPOT"):
#   train_squad_examples = create_squadHotPot_examples(raw_train_data)
# else:
#   train_squad_examples = create_squad_examples(raw_train_data)

# x_train, y_train = create_inputs_targets(train_squad_examples)
# print(f"{len(train_squad_examples)} training points created.")


# if (parser == "COQA"):
#   eval_squad_examples = create_coqa_examples(raw_eval_data)
# elif (parser == "HOTPOT"):
#   eval_squad_examples = create_squadHotPot_examples(raw_eval_data)
# else:
#   eval_squad_examples = create_squad_examples(raw_eval_data)


# x_eval, y_eval = create_inputs_targets(eval_squad_examples)
# print(f"{len(eval_squad_examples)} evaluation points created.")
start_logits = layers.Dense(1, name="start_logit", use_bias=False)(sequence_output)
start_logits = layers.Flatten()(start_logits)
end_logits = layers.Dense(1, name="end_logit", use_bias=False)(sequence_output)
end_logits = layers.Flatten()(end_logits)
start_probs = layers.Activation(keras.activations.softmax)(start_logits)
end_probs = layers.Activation(keras.activations.softmax)(end_logits)
model = keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=[start_probs, end_probs])
loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)
optimizer = keras.optimizers.Adam(lr=1e-5, beta_1=0.9, beta_2=0.98, epsilon=1e-9)
model.compile(optimizer=optimizer, loss=[loss, loss])
model.summary()

# Restore the weights
model.load_weights("/content/gdrive/My Drive/TrainedResults/SQUAD2.0SeqLength512CorrectUnanswerable.h5")


# model.fit(x_train, y_train, epochs=2, batch_size=8, callbacks=[ValidationCallback(x_eval, y_eval)])
# model.save_weights("/content/gdrive/My Drive/TrainingResults/weights.h5")
# ==================================================== TESTING =========================================================
data = {"data":
    [
        {"title": "Project Apollo",
         "paragraphs": [
             {
                 "context": "The Apollo program, also known as Project Apollo, was the third United States human "
                            "spaceflight program carried out by the National Aeronautics and Space Administration ("
                            "NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First "
                            "conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to "
                            "follow the one-man Project Mercury which put the first Americans in space, Apollo was "
                            "later dedicated to President John F. Kennedy's national goal of landing a man on the "
                            "Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in "
                            "a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project "
                            "Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, "
                            "and was supported by the two man Gemini program which ran concurrently with it from 1962 "
                            "to 1966. Gemini missions developed some of the space travel techniques that were "
                            "necessary for the success of the Apollo missions. Apollo used Saturn family rockets as "
                            "launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications "
                            "Program, which consisted of Skylab, a space station that supported three manned missions "
                            "in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the "
                            "Soviet Union in 1975.",
                 "qas": [
                     {"question": "What project put the first Americans into space?",
                      "id": "Q1"
                      },
                     {"question": "What program was created to carry out these projects and missions?",
                      "id": "Q2"
                      },
                     {"question": "What year did the first manned Apollo flight occur?",
                      "id": "Q3"
                      },
                     {"question": "What President is credited with the original notion of putting Americans in space?",
                      "id": "Q4"
                      },
                     {"question": "Who did the U.S. collaborate with on an Earth orbit mission in 1975?",
                      "id": "Q5"
                      },
                     {"question": "How long did Project Apollo run?",
                      "id": "Q6"
                      },
                     {"question": "What program helped develop space travel techniques that Project Apollo used?",
                      "id": "Q7"
                      },
                     {"question": "What space station supported three manned missions in 1973-1974?",
                      "id": "Q8"
                      }
                 ]}]}]}

test_samples = create_squad_examples(data)
x_test, _ = create_inputs_targets(test_samples)
pred_start, pred_end = model.predict(x_test)
for idx, (start, end) in enumerate(zip(pred_start, pred_end)):
    test_sample = test_samples[idx]
    offsets = test_sample.context_token_to_char
    start = np.argmax(start)
    end = np.argmax(end)
    pred_ans = None
    if start >= len(offsets):
        continue
    pred_char_start = offsets[start][0]
    if (pred_char_start == -1):
      pred_ans = "Unanswerable"
    elif end < len(offsets):
        pred_ans = test_sample.context[pred_char_start:offsets[end][1]]
    else:
        pred_ans = test_sample.context[pred_char_start:]
    print("Q: " + test_sample.question)
    print("A: " + pred_ans)
# ==================================================== ORIGINAL TESTING =========================================================


test_samples = create_squad_examples(raw_eval_data)
x_test, _ = create_inputs_targets(test_samples)
pred_start, pred_end = model.predict(x_test)
for idx, (start, end) in enumerate(zip(pred_start, pred_end)):
    test_sample = test_samples[idx]
    #print(test_sample)
    offsets = test_sample.context_token_to_char
    start = np.argmax(start)
    end = np.argmax(end)
    pred_ans = None
    if start >= len(offsets):
        continue
    pred_char_start = offsets[start][0]
    if end < len(offsets):
        pred_ans = test_sample.context[pred_char_start:offsets[end][1]]
    else:
        pred_ans = test_sample.context[pred_char_start:]
    test_sample.answer_text = pred_ans
    test_sample.start_char_idx = pred_char_start
    # print("Q: " + test_sample.question)
    # print("A: " + pred_ans)

print(samples_to_predictions(test_samples))
print(samples_to_squad_json(test_samples))
